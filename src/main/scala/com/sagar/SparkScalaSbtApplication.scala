package com.sagar

import org.apache.spark.sql.SparkSession

object SparkScalaSbtApplication {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder
      .appName("Spark Scala SBT Example")
      .master("local[*]")

      .config("spark.driver.bindAddress", "localhost") // ðŸ‘ˆ Bind to localhost

      .getOrCreate()

    // Set log level to reduce Spark logging
    spark.sparkContext.setLogLevel("ERROR")

    // âœ… Read CSV file
    val filePath = "src/main/resources/data/employees.csv"
    val df = spark.read
      .option("header", "true") // âœ… First row as header
      .option("inferSchema", "true") // âœ… Automatically infer data types
      .csv(filePath)

    // âœ… Show DataFrame
    df.show()

    // âœ… Print schema
    df.printSchema()

    // âœ… Filter example - Employees with salary > 65000
    df.filter("salary > 65000").show()

    // âœ… Group by example - Average salary per department
    df.groupBy("department")
      .avg("salary")
      .show()
    println("Spark UI: " + spark.sparkContext.uiWebUrl.getOrElse("Not started"))

    // ðŸ‘‡ Keep SparkSession alive
    println("Spark application is running. Press Ctrl+C to stop.")
    Thread.sleep(Long.MaxValue)

    // âœ… Stop Spark session
    spark.stop()
  }
}
